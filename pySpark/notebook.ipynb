{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "israeli-glass",
   "metadata": {},
   "source": [
    "# Football Twitter Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-quarter",
   "metadata": {},
   "source": [
    "## Imports the needed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "flying-naples",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import lit, explode, split, col, from_json, to_json, json_tuple, window, struct, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, LongType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-effect",
   "metadata": {},
   "source": [
    "## Set Up Spark Session and Define Schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-database",
   "metadata": {},
   "source": [
    "The schemas defined here are needed in order to extract the content from the twitter posts in the kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "colored-moment",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"wordCounter\") \\\n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1') \\\n",
    "    .getOrCreate() \n",
    "\n",
    "# Defines schema of Twitter Post\n",
    "tweetSchema = StructType() \\\n",
    "    .add(\"payload\", StringType())\n",
    "\n",
    "payloadSchema = StructType() \\\n",
    "    .add(\"Text\", StringType()) \\\n",
    "    .add(\"Lang\", StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-minister",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-province",
   "metadata": {},
   "source": [
    "ExtractTweetPayload is a function used to extract a dataframe with the content and timestamp of the twitter post from a kafka message in json format.\n",
    "\n",
    "GetLastName is a UDF used to extract the last word in a string column of a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "negative-participation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts structured content from json tweet message\n",
    "def extractTweetPayload(df, tweetSchema, payloadSchema):\n",
    "    return df \\\n",
    "        .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"CAST(timestamp AS TIMESTAMP)\", \"offset\") \\\n",
    "        .withColumn(\"data\", from_json(\"value\", tweetSchema)) \\\n",
    "        .withColumn(\"payload\", from_json(\"data.payload\", payloadSchema)) \\\n",
    "        .select(\"payload.*\", \"key\", \"timestamp\")\n",
    "\n",
    "\n",
    "def getLastName(full_name):\n",
    "    return full_name.split(\" \")[-1:][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-edgar",
   "metadata": {},
   "source": [
    "## Streaming Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-hundred",
   "metadata": {},
   "source": [
    "Here I define the streaming queries, they perform simple word count over specific columns of the kafka messages. \n",
    "I decided to go with hopping windows in order to visualize in a more efficient ways moving trends in the Twitter topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "stunning-ensemble",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweetsCountQuery(df):\n",
    "    return df \\\n",
    "        .withWatermark(\"timestamp\", \"2 minutes\") \\\n",
    "        .count() \\\n",
    "        .select(\"count\", to_json(struct(\"count\")).alias(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sticky-citizen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordCountQuery(df, colName):\n",
    "    return df \\\n",
    "        .withWatermark(\"timestamp\", \"10 seconds\") \\\n",
    "        .withColumn('word', explode(split(col(colName), ' '))) \\\n",
    "        .groupBy(window(col(\"timestamp\"), \"10 seconds\", \"5 seconds\"),\n",
    "                 col('word')\n",
    "                 ).count() \\\n",
    "        .select(\"word\", \"count\", to_json(struct(\"word\", \"count\")).alias(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "exempt-disorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def langCountQuery(df, colName):\n",
    "    return df \\\n",
    "        .withWatermark(\"timestamp\", \"2 minutes\") \\\n",
    "        .groupBy(\n",
    "            window(col(\"timestamp\"), \"2 minutes\", \"1 minutes\"),\n",
    "            col(colName)\n",
    "        ).count() \\\n",
    "        .select(colName, \"count\", to_json(struct(colName, \"count\")).alias(\"value\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-metropolitan",
   "metadata": {},
   "source": [
    "## Static Dataset Import and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-deficit",
   "metadata": {},
   "source": [
    "In order to extract the meaningful words from the twitter posts,I decided to load the public FIFA 21 dataset, which contains data about the most popular football players and clubs.\n",
    "In the following lines I load the dataset, then I extract and concatenate the list of teams and players into an unique dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afraid-typing",
   "metadata": {},
   "outputs": [],
   "source": [
    "players = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"mode\", \"DROPMALFORMED\") \\\n",
    "    .csv(\"players_21.csv\")\n",
    "\n",
    "lastNameUDF = udf(getLastName, StringType())\n",
    "\n",
    "player_names = players \\\n",
    "    .withColumn(\n",
    "        \"word\", lastNameUDF(\"short_name\")) \\\n",
    "    .withColumn(\"category\", lit(\"Player\")) \\\n",
    "    .select(\"word\", \"category\") \\\n",
    "    .limit(500) \\\n",
    "\n",
    "teams = players \\\n",
    "    .select(\"club_name\") \\\n",
    "    .withColumn(\"category\", lit(\"Team\")) \\\n",
    "    .limit(500) \\\n",
    "    .dropDuplicates() \\\n",
    "\n",
    "topics = player_names.union(teams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-visit",
   "metadata": {},
   "source": [
    "# Kafka Data Source Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "executive-programmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the data from kafka\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"broker:9092\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .option(\"subscribe\", \"tweets\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "messages = extractTweetPayload(df, tweetSchema, payloadSchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-zoning",
   "metadata": {},
   "source": [
    "# Streaming Queries Startup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-metabolism",
   "metadata": {},
   "source": [
    "Run the following cells to start the streaming queries and write the final output into the respective Kafka topics \n",
    "\n",
    "Note that the queries will keep on running until you stop them,since they're operating on a never ending stream ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "announced-collector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the topics are counted from the queries and joined with the players and clubs dataframe \n",
    "wordCount = wordCountQuery(messages, \"Text\") \\\n",
    "    .join(topics, \"word\") \\\n",
    "    .select(\"word\", \"count\",\"category\", to_json(struct(\"word\", \"count\",\"category\")).alias(\"value\"))\n",
    "\n",
    "# the final output is written to the CountByName Kafka topic\n",
    "wordCountStreamQuery = wordCount \\\n",
    "    .writeStream\\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"checkpointLocation\", \"./checkpoints\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"broker:9092\") \\\n",
    "    .option(\"topic\", \"countByName\") \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "sweet-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCountStreamQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "focal-supervision",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table or view not found: results; line 1 pos 14;\n'Sort ['timestamp DESC NULLS LAST], true\n+- 'Project [*]\n   +- 'UnresolvedRelation [results], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d1c239011002>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM results ORDER BY timestamp DESC\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \"\"\"\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table or view not found: results; line 1 pos 14;\n'Sort ['timestamp DESC NULLS LAST], true\n+- 'Project [*]\n   +- 'UnresolvedRelation [results], [], false\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM results ORDER BY timestamp DESC\").show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "reflected-beads",
   "metadata": {},
   "outputs": [],
   "source": [
    "langCount = langCountQuery(messages, \"Lang\")\n",
    "\n",
    "langCountStreamQuery = langCount \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"checkpointLocation\", \"./checkpoints\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"broker:9092\") \\\n",
    "    .option(\"topic\", \"countByLang\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "minimal-basis",
   "metadata": {},
   "outputs": [],
   "source": [
    "langCountStreamQuery.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
